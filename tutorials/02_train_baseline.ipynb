{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Baseline\n",
    "\n",
    "This notebook shows how to train the baseline model for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from utils.train import TrainConfig, run_train_model\n",
    "from utils.augmentations import get_default_transform\n",
    "from utils import creating_dataset\n",
    "\n",
    "# this is the implementation of the custom baseline model\n",
    "from utils import hvatnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainer configuration\n",
    "\n",
    "The `TrainConfig` class is used to train the baseline model - have a look at the parameters it has!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(exp_name='test_2_run_fedya', p_augs=0.3, batch_size=64, eval_interval=150, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting val datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Getting train datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Number of moves: 70 | Dataset: valery_first_standart_elbow_left\n",
      "Reorder this dataset valery_first_standart_elbow_left True\n",
      "Number of moves: 135 | Dataset: alex_kovalev_standart_elbow_left\n",
      "Reorder this dataset alex_kovalev_standart_elbow_left True\n",
      "Number of moves: 72 | Dataset: anna_makarova_standart_elbow_left\n",
      "Reorder this dataset anna_makarova_standart_elbow_left True\n",
      "Number of moves: 62 | Dataset: artem_snailbox_standart_elbow_left\n",
      "Reorder this dataset artem_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: matthew_antonov_standart_elbow_left\n",
      "Reorder this dataset matthew_antonov_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_left\n",
      "Reorder this dataset misha_korobok_standart_elbow_left True\n",
      "Number of moves: 71 | Dataset: nikita_snailbox_standart_elbow_left\n",
      "Reorder this dataset nikita_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: petya_chizhov_standart_elbow_left\n",
      "Reorder this dataset petya_chizhov_standart_elbow_left True\n",
      "Number of moves: 12 | Dataset: polina_maksimova_standart_elbow_left\n",
      "Reorder this dataset polina_maksimova_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: sema_duplin_standart_elbow_left\n",
      "Reorder this dataset sema_duplin_standart_elbow_left True\n",
      "Number of moves: 136 | Dataset: alex_kovalev_standart_elbow_right\n",
      "Number of moves: 69 | Dataset: andrew_snailbox_standart_elbow_right\n",
      "Number of moves: 132 | Dataset: anna_makarova_standart_elbow_right\n",
      "Number of moves: 67 | Dataset: artem_snailbox_standart_elbow_right\n",
      "Number of moves: 68 | Dataset: matthew_antonov_standart_elbow_right\n",
      "Number of moves: 72 | Dataset: matvey_gorbenko_standart_elbow_right\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_right\n",
      "Number of moves: 55 | Dataset: nikita_snailbox_standart_elbow_right\n",
      "Number of moves: 142 | Dataset: petya_chizhov_standart_elbow_right\n",
      "Number of moves: 54 | Dataset: polina_maksimova_standart_elbow_right\n",
      "Number of moves: 139 | Dataset: sema_duplin_standart_elbow_right\n",
      "Number of trainining sessions: 22\n",
      "Number of validation sessions: 1\n",
      "Size of the input (8, 256) || Size of the output (20, 32)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"C:\\Users\\nodos\\Desktop\\ДИПЛОМ\\Аня диплом\\dataset_v2_blocks\\dataset_v2_blocks\"\n",
    "\n",
    "def count_parameters(model): \n",
    "    n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n_total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total: {n_total/1e6:.2f}M, Trainable: {n_trainable/1e6:.2f}M\")\n",
    "    return n_total, n_trainable\n",
    "\n",
    "\n",
    "    \n",
    "## Data preparation\n",
    "transform = get_default_transform(train_config.p_augs)\n",
    "data_paths = dict(datasets=[DATA_PATH],\n",
    "                    hand_type = ['left', 'right'], # [left, 'right']\n",
    "                    human_type = ['health', 'amputant'], # [amputant, 'health']\n",
    "                    test_dataset_list = ['fedya_tropin_standart_elbow_left'])\n",
    "data_config = creating_dataset.DataConfig(**data_paths)\n",
    "train_dataset, test_dataset = creating_dataset.get_datasets(data_config, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model\n",
    "As you can see below, the model has a number of hyperparameters specifying its architecture and parameters. These are the parameters used to generate the baseline predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4210788\n",
      "Total: 4.21M, Trainable: 4.21M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4210788, 4210788)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = hvatnet.Config(n_electrodes=8, n_channels_out=20,\n",
    "                            n_res_blocks=3, n_blocks_per_layer=3,\n",
    "                            n_filters=128, kernel_size=3,\n",
    "                            strides=(2, 2, 2), dilation=2, \n",
    "                            small_strides = (2, 2))\n",
    "model = hvatnet.HVATNetv3(model_config)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the predictions are downsampled at 25Hz from the data originally recorded at 200Hz. The `hvatnet` model used here, automatically and correctly downsamples the data during predictions. Make sure that your model's oputput is also downsampled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8, 256), Y shape: (20, 32)\n",
      "Predictions shape: (20, 32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = train_dataset[0]\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "\n",
    "Y_hat = model(torch.tensor(X).unsqueeze(0)).squeeze().detach().numpy()\n",
    "\n",
    "print(f\"Predictions shape: {Y_hat.shape}\")\n",
    "\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains the baseline model using training code defined in `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed initialization of scheduler\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 150: 0.3274211287498474\n",
      "val loss: 0.3714042901992798\n",
      "saved model:  step_150_loss_0.3714.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 300: 0.24301214516162872\n",
      "val loss: 0.3670700788497925\n",
      "saved model:  step_300_loss_0.3671.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 450: 0.24274182319641113\n",
      "val loss: 0.3388451337814331\n",
      "saved model:  step_450_loss_0.3388.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 600: 0.2532549500465393\n",
      "val loss: 0.33789315819740295\n",
      "saved model:  step_600_loss_0.3379.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 750: 0.2477158010005951\n",
      "val loss: 0.3100314438343048\n",
      "saved model:  step_750_loss_0.3100.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 900: 0.23761801421642303\n",
      "val loss: 0.3461247682571411\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1050: 0.2836146950721741\n",
      "val loss: 0.34211447834968567\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1200: 0.22716495394706726\n",
      "val loss: 0.32885366678237915\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1350: 0.2352919578552246\n",
      "val loss: 0.30896705389022827\n",
      "saved model:  step_1350_loss_0.3090.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1500: 0.22555962204933167\n",
      "val loss: 0.31575900316238403\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1650: 0.2114012986421585\n",
      "val loss: 0.29195305705070496\n",
      "saved model:  step_1650_loss_0.2920.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1800: 0.23245230317115784\n",
      "val loss: 0.3325604200363159\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1950: 0.2536258399486542\n",
      "val loss: 0.3123852014541626\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2100: 0.2645226716995239\n",
      "val loss: 0.32528451085090637\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2250: 0.25758498907089233\n",
      "val loss: 0.319775253534317\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2400: 0.24784550070762634\n",
      "val loss: 0.2942471206188202\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2550: 0.21510858833789825\n",
      "val loss: 0.3298601806163788\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2700: 0.23588308691978455\n",
      "val loss: 0.3011874854564667\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2850: 0.22495393455028534\n",
      "val loss: 0.2802709937095642\n",
      "saved model:  step_2850_loss_0.2803.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3000: 0.2268258035182953\n",
      "val loss: 0.29517585039138794\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3150: 0.2209663838148117\n",
      "val loss: 0.27693718671798706\n",
      "saved model:  step_3150_loss_0.2769.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3300: 0.21528711915016174\n",
      "val loss: 0.3368089497089386\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3450: 0.23444512486457825\n",
      "val loss: 0.2757587730884552\n",
      "saved model:  step_3450_loss_0.2758.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3600: 0.23156681656837463\n",
      "val loss: 0.2951982319355011\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3750: 0.2269786149263382\n",
      "val loss: 0.2769438922405243\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3900: 0.23634347319602966\n",
      "val loss: 0.2888075113296509\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4050: 0.23881034553050995\n",
      "val loss: 0.2823086977005005\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4200: 0.21289169788360596\n",
      "val loss: 0.2870161831378937\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4350: 0.22127752006053925\n",
      "val loss: 0.2820160984992981\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4500: 0.22256414592266083\n",
      "val loss: 0.2903624176979065\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4650: 0.2197410613298416\n",
      "val loss: 0.27692896127700806\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4800: 0.22597169876098633\n",
      "val loss: 0.2880997359752655\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4950: 0.19838190078735352\n",
      "val loss: 0.29274627566337585\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5100: 0.22253207862377167\n",
      "val loss: 0.28189682960510254\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5250: 0.22155146300792694\n",
      "val loss: 0.2811669111251831\n",
      "\n",
      "\n",
      "*****************************************************"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrun_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\amputants\\utils\\train.py:113\u001b[0m, in \u001b[0;36mrun_train_model\u001b[1;34m(model, datasets, config, device)\u001b[0m\n\u001b[0;32m    111\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    112\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m model(inputs, labels)\n\u001b[1;32m--> 113\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mgrad_clip:\n\u001b[0;32m    116\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_clip)\n",
      "File \u001b[1;32mc:\\Users\\nodos\\PycharmProjects\\amputants\\.venv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nodos\\PycharmProjects\\amputants\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nodos\\PycharmProjects\\amputants\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "run_train_model(model, (train_dataset, test_dataset), train_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
